import os
from typing import List
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from document_processor import LegalDocumentProcessor
import traceback

# Load environment variables
load_dotenv()

class LegalRAGSystem:
    def __init__(self):
        self.google_api_key = os.getenv("GOOGLE_API_KEY")

        embedding_provider = os.getenv("EMBEDDING_PROVIDER", "huggingface").lower()
        if embedding_provider == "google" and self.google_api_key:
            embedding_model = os.getenv("GOOGLE_EMBEDDING_MODEL", "models/embedding-001")
            print(f"üîÑ Using Google embeddings model: {embedding_model}")
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model=embedding_model,
                google_api_key=self.google_api_key
            )
        else:
            embedding_model = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
            print(f"üîÑ Using HuggingFace embeddings model: {embedding_model}")
            self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

        llm_provider = os.getenv("LLM_PROVIDER", "google").lower()
        temperature = float(os.getenv("LLM_TEMPERATURE", "0.1"))

        if llm_provider == "google":
            if not self.google_api_key:
                raise ValueError("GOOGLE_API_KEY is required when LLM_PROVIDER=google.")
            chat_model = os.getenv("GOOGLE_CHAT_MODEL", "gemini-1.5-flash-8b")
            print(f"üîÑ Using Google chat model: {chat_model}")
            self.llm = ChatGoogleGenerativeAI(
                model=chat_model,
                temperature=temperature,
                google_api_key=self.google_api_key
            )
        elif llm_provider == "ollama":
            chat_model = os.getenv("OLLAMA_MODEL", "llama3.1")
            print(f"üîÑ Using Ollama chat model: {chat_model}")
            self.llm = ChatOllama(
                model=chat_model,
                temperature=temperature
            )
        else:
            raise ValueError("Unsupported LLM_PROVIDER. Use 'google' or 'ollama'.")

        self.vector_store = None

        # Prompt template
        self.legal_prompt = PromptTemplate(
            template="""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC:
1. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p
2. N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c vƒÉn b·∫£n ph√°p lu·∫≠t hi·ªán c√≥"
3. Tr√≠ch d·∫´n c·ª• th·ªÉ ƒëi·ªÅu, kho·∫£n li√™n quan n·∫øu c√≥
4. S·ª≠ d·ª•ng ng√¥n ng·ªØ ph√°p lu·∫≠t ch√≠nh x√°c v√† d·ªÖ hi·ªÉu
5. ƒê∆∞a ra l·ªùi khuy√™n th·∫≠n tr·ªçng, khuy·∫øn kh√≠ch tham kh·∫£o lu·∫≠t s∆∞ n·∫øu c·∫ßn

VƒÉn b·∫£n ph√°p lu·∫≠t tham kh·∫£o:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:""",
            input_variables=["context", "question"]
        )

    def build_knowledge_base(self, data_folder: str = "data"):
        print("üîÑ ƒêang x·ª≠ l√Ω t√†i li·ªáu ph√°p lu·∫≠t...")
        if not os.path.exists(data_folder):
            raise ValueError(f"Th∆∞ m·ª•c {data_folder} kh√¥ng t·ªìn t·∫°i!")
        files = os.listdir(data_folder)
        if not files:
            raise ValueError(f"Kh√¥ng c√≥ file n√†o trong th∆∞ m·ª•c {data_folder}!")
        print(f"üìÅ T√¨m th·∫•y {len(files)} file trong th∆∞ m·ª•c data: {files}")

        try:
            processor = LegalDocumentProcessor()
            documents = processor.process_documents(data_folder)
            if not documents:
                raise ValueError("Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu n√†o!")
            print(f"üìö ƒê√£ x·ª≠ l√Ω {len(documents)} chunks t·ª´ t√†i li·ªáu ph√°p lu·∫≠t")

            # T·∫°o vector store
            print("üîÑ ƒêang t·∫°o vector database...")
            self.vector_store = FAISS.from_documents(documents, self.embeddings)
            os.makedirs("vectorstore", exist_ok=True)
            print("üíæ ƒêang l∆∞u vector database...")
            self.vector_store.save_local("vectorstore/legal_faiss")
            print("‚úÖ Knowledge base ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªói khi x√¢y d·ª±ng knowledge base: {e}")
            traceback.print_exc()
            raise e

    def load_knowledge_base(self):
        vectorstore_path = "vectorstore/legal_faiss"
        if not os.path.exists(vectorstore_path):
            print("‚ùå Kh√¥ng t√¨m th·∫•y vectorstore. C·∫ßn x√¢y d·ª±ng knowledge base.")
            return False
        try:
            required_files = ["index.faiss", "index.pkl"]
            for file in required_files:
                if not os.path.exists(os.path.join(vectorstore_path, file)):
                    print(f"‚ùå Thi·∫øu file: {file}")
                    return False
            print("üîÑ ƒêang load vectorstore...")
            self.vector_store = FAISS.load_local(
                vectorstore_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("‚úÖ ƒê√£ load knowledge base th√†nh c√¥ng!")
            return True
        except Exception as e:
            print(f"‚ùå Kh√¥ng th·ªÉ load knowledge base: {e}")
            traceback.print_exc()
            return False

    def query(self, question: str) -> dict:
        if not self.vector_store:
            return {"answer": "H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. Vui l√≤ng x√¢y d·ª±ng knowledge base tr∆∞·ªõc.", "sources": []}
        try:
            retrieved_docs = self.vector_store.similarity_search(question, k=5)
            if not retrieved_docs:
                return {
                    "answer": "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c vƒÉn b·∫£n ph√°p lu·∫≠t hi·ªán c√≥.",
                    "sources": []
                }

            context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
            prompt = self.legal_prompt.format(context=context_text, question=question)
            response = self.llm.invoke(prompt)

            if hasattr(response, "content"):
                answer_text = response.content
            else:
                answer_text = str(response)

            if not answer_text or answer_text.strip() == "":
                answer_text = "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c√°c vƒÉn b·∫£n ph√°p lu·∫≠t hi·ªán c√≥."

            sources = []
            for i, doc in enumerate(retrieved_docs):
                sources.append({
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source", "Unknown"),
                    "chunk_id": doc.metadata.get("chunk_index", i),
                    "page": doc.metadata.get("page_number", "N/A")
                })

            return {"answer": answer_text, "sources": sources}
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
            traceback.print_exc()
            return {"answer": f"C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi: {e}", "sources": []}

    def debug_chain_inputs(self, question: str, k: int = 5) -> dict:
        if not self.vector_store:
            raise RuntimeError("Knowledge base ch∆∞a s·∫µn s√†ng. Vui l√≤ng x√¢y d·ª±ng ho·∫∑c load tr∆∞·ªõc.")

        retrieved_docs = self.vector_store.similarity_search(question, k=k)
        context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
        prompt = self.legal_prompt.format(context=context_text, question=question)

        debug_docs = []
        for i, doc in enumerate(retrieved_docs):
            debug_docs.append({
                "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                "source": doc.metadata.get("source", "Unknown"),
                "chunk_index": doc.metadata.get("chunk_index", i),
                "page_number": doc.metadata.get("page_number", "N/A")
            })

        return {
            "question": question,
            "retrieved_documents": debug_docs,
            "prompt": prompt
        }

    def get_related_articles(self, query: str, k: int = 3) -> List[dict]:
        if not self.vector_store:
            return []
        try:
            docs = self.vector_store.similarity_search(query, k=k)
            return [
                {
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", "Unknown"),
                    "similarity_score": "High",
                    "page": doc.metadata.get("page_number", "N/A")
                }
                for doc in docs
            ]
        except Exception as e:
            print(f"L·ªói khi t√¨m ƒëi·ªÅu lu·∫≠t li√™n quan: {e}")
            return []


if __name__ == "__main__":
    rag = LegalRAGSystem()
    if not rag.load_knowledge_base():
        rag.build_knowledge_base("data")
    question = "Quy ƒë·ªãnh v·ªÅ th·ªùi h·∫°n s·ª≠ d·ª•ng ƒë·∫•t n√¥ng nghi·ªáp l√† g√¨?"
    answer = rag.query(question)
    print("\n=== C√ÇU TR·∫¢ L·ªúI ===")
    print(answer["answer"])
    print("\n=== NGU·ªíN ===")
    for src in answer["sources"]:
        print(f"- {src['source']} (chunk {src['chunk_id']}, page {src['page']})")
