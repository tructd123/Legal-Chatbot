import os
import shutil
import PyPDF2
import pdfplumber
import fitz
from docx import Document
from pdf2image import convert_from_path
import pytesseract
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document as LangchainDocument
from typing import List, Tuple, Union
from tempfile import TemporaryDirectory
from PIL import ImageFilter, ImageOps

class LegalDocumentProcessor:
    def __init__(self): 
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
    
    def read_pdf(self, file_path: str, return_pages: bool = False) -> Union[str, Tuple[str, List[str]]]: 
        text_pages: list[str] = []
        missing_pages: list[int] = []
        total_pages = 0

        # --- C√ÅCH 1: TH·ª¨ PyMuPDF (fitz) ƒê·∫¶U TI√äN (R·∫•t m·∫°nh) ---
        try:
            with fitz.open(file_path) as doc:
                total_pages = len(doc)
                for idx in range(total_pages):
                    page = doc.load_page(idx)
                    page_text = page.get_text("text", sort=True) or ""
                    text_pages.append(page_text)
                    if not page_text.strip():
                        missing_pages.append(idx)
            
            if text_pages and not all(p.strip() == "" for p in text_pages):
                print(f"‚úÖ Extracted text with PyMuPDF ({sum(len(p) for p in text_pages)} chars)")
            else:
                # PyMuPDF kh√¥ng l·∫•y ƒë∆∞·ª£c g√¨, reset ƒë·ªÉ th·ª≠ c√°ch kh√°c
                text_pages = []
                missing_pages = []
                total_pages = 0
                raise Exception("PyMuPDF extracted no text.") # Chuy·ªÉn sang Pypdfplumber
                
        except Exception as e:
            print(f"‚ö†Ô∏è PyMuPDF failed: {e}. Trying pdfplumber...")
            # --- H·∫æT C√ÅCH 1 ---

            # Th·ª≠ pdfplumber (Code c≈© c·ªßa b·∫°n, kh√¥ng ƒë·ªïi)
            try:
                with pdfplumber.open(file_path) as pdf:
                    total_pages = len(pdf.pages)
                    for idx, page in enumerate(pdf.pages):
                        page_text = page.extract_text() or ""
                        text_pages.append(page_text)
                        if not page_text.strip():
                            missing_pages.append(idx)
                if text_pages and not all(p.strip() == "" for p in text_pages):
                     print(f"‚úÖ Extracted text with pdfplumber ({sum(len(p) for p in text_pages)} chars)")
                else:
                    raise Exception("pdfplumber extracted no text.")
            except Exception as e:
                print(f"‚ö†Ô∏è pdfplumber failed: {e}. Trying PyPDF2...")
                text_pages = []
                missing_pages = []
                total_pages = 0
                
                # Fallback to PyPDF2 (Code c≈© c·ªßa b·∫°n, kh√¥ng ƒë·ªïi)
                try:
                    with open(file_path, 'rb') as file:
                        reader = PyPDF2.PdfReader(file)
                        total_pages = len(reader.pages)
                        if total_pages == 0: # X·ª≠ l√Ω file PDF b·ªã m√£ h√≥a
                            raise Exception("PyPDF2 could not read pages (possibly encrypted).")
                        text_pages = ["" for _ in range(total_pages)] # Kh·ªüi t·∫°o list
                        for idx, page in enumerate(reader.pages):
                            page_text = page.extract_text() or ""
                            text_pages[idx] = page_text
                            if not page_text.strip():
                                missing_pages.append(idx)
                    if text_pages and not all(p.strip() == "" for p in text_pages):
                        print(f"‚úÖ Extracted text with PyPDF2 ({sum(len(p) for p in text_pages)} chars)")
                    else:
                         missing_pages = list(range(total_pages)) # To√†n b·ªô ƒë·ªÅu r·ªóng
                except Exception as e:
                    print(f"‚ö†Ô∏è PyPDF2 failed: {e}. Will attempt OCR.")
                    text_pages = []
                    missing_pages = []
                    total_pages = 0


        # --- C√ÅCH 2: FALLBACK TO OCR (ƒê√É T·ªêI ∆ØU H√ìA) ---
        poppler_path = os.getenv("POPPLER_PATH")
        tesseract_cmd = os.getenv("TESSERACT_CMD")
        if tesseract_cmd:
            pytesseract.pytesseract.tesseract_cmd = tesseract_cmd

        # N·∫øu kh√¥ng c√≥ text_pages (PyMuPDF/Plumber/PDF2 ƒë·ªÅu th·∫•t b·∫°i) 
        # HO·∫∂C c√≥ m·ªôt s·ªë trang b·ªã thi·∫øu text
        needs_ocr = not text_pages or missing_pages

        if needs_ocr:
            print(f"‚ÑπÔ∏è Attempting OCR fallback...")
            try:
                # N·∫øu text_pages r·ªóng, ch√∫ng ta c·∫ßn bi·∫øt t·ªïng s·ªë trang
                if total_pages == 0:
                    try:
                        # M·ªü l·∫°i b·∫±ng PyPDF2 ch·ªâ ƒë·ªÉ ƒë·∫øm trang
                        with open(file_path, 'rb') as file:
                            reader = PyPDF2.PdfReader(file)
                            total_pages = len(reader.pages)
                    except Exception:
                        # N·∫øu v·∫´n l·ªói, th·ª≠ ƒë·∫øm b·∫±ng pdfplumber
                        try:
                            with pdfplumber.open(file_path) as pdf:
                                total_pages = len(pdf.pages)
                        except Exception as e:
                            print(f"‚ùå Cannot determine page count for OCR: {e}")
                            return "" # B·ªè cu·ªôc

                if not text_pages:
                    text_pages = ["" for _ in range(total_pages)]
                    missing_pages = list(range(total_pages))
                
                print(f"‚ÑπÔ∏è OCR-ing {len(missing_pages)} pages (one by one)...")
                
                ocr_dpi = int(os.getenv("PDF_OCR_DPI", "300")) # Gi·∫£m DPI trong .env n·∫øu v·∫´n ch·∫≠m
                ocr_lang = os.getenv("PDF_OCR_LANG", "vie+eng")
                ocr_config = os.getenv("PDF_OCR_CONFIG", "")

                kwargs_poppler = {"poppler_path": poppler_path} if poppler_path else {}

                verbose_ocr = os.getenv("PDF_OCR_VERBOSE", "0") == "1"

                for page_idx in missing_pages:
                    # [T·ªêI ∆ØU H√ìA] Ch·ªâ chuy·ªÉn ƒë·ªïi 1 trang t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
                    try:
                        image = convert_from_path(
                            file_path,
                            dpi=ocr_dpi,
                            first_page=page_idx + 1, # pdf2image d√πng index 1
                            last_page=page_idx + 1,
                            fmt="png",
                            **kwargs_poppler
                        )[0] # L·∫•y ·∫£nh duy nh·∫•t trong list

                        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh (code c≈© c·ªßa b·∫°n)
                        img = image.convert("L")
                        img = ImageOps.equalize(img)
                        img = img.filter(ImageFilter.MedianFilter())
                        
                        page_text = pytesseract.image_to_string(
                            img,
                            lang=ocr_lang,
                            config=ocr_config
                        )
                        page_text = page_text.replace("\x0c", "").strip()
                        text_pages[page_idx] = page_text
                        if verbose_ocr:
                            char_count = len(page_text)
                            status = "chars" if char_count else "empty"
                            print(f"   ... OCR page {page_idx+1}: {char_count} {status}")
                        # print(f"   ... OCR page {page_idx+1} complete.") # B·ªè comment n·∫øu mu·ªën xem ti·∫øn tr√¨nh
                        
                    except Exception as page_e:
                        print(f"‚ö†Ô∏è Failed to OCR page {page_idx+1}: {page_e}")
                        text_pages[page_idx] = "" # ƒê√°nh d·∫•u l√† r·ªóng n·∫øu l·ªói

                print(f"‚úÖ OCR complete. Total chars: ({sum(len(p) for p in text_pages)})")
            
            except Exception as e:
                # Kh·ªëi ch·∫©n ƒëo√°n c·ªßa b·∫°n (gi·ªØ nguy√™n)
                pdfinfo_path = shutil.which("pdfinfo")
                print(f"‚ö†Ô∏è OCR fallback failed entirely: {e}")
                if not poppler_path and not pdfinfo_path:
                     print("‚ÑπÔ∏è G·ª£i √Ω: C√†i Poppler...")
                # ... (gi·ªØ nguy√™n c√°c g·ª£i √Ω kh√°c) ...

        # K·∫øt h·ª£p text cu·ªëi c√πng
        combined_text = "\n".join(page_text for page_text in text_pages if page_text.strip())
        if combined_text.strip():
            return (combined_text, text_pages) if return_pages else combined_text

        print(f"‚ùå No text extracted from {file_path}. All methods failed.")
        return ("", text_pages) if return_pages else ""
    
    def read_docx(self, file_path: str) -> str:
        doc = Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs).replace("\ufeff", "")
    
    def read_txt(self, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
            return file.read().replace("\ufeff", "")

    def process_documents(self, data_folder: str) -> List[LangchainDocument]:
        documents = []
        total_chunks = 0
        
        for filename in os.listdir(data_folder):
            file_path = os.path.join(data_folder, filename)
            ext = filename.split('.')[-1].lower()

            try: 
                if filename.endswith('.pdf'):
                    text = self.read_pdf(file_path)
                elif filename.endswith('.docx'):
                    text = self.read_docx(file_path)
                elif filename.endswith('.txt'):
                    text = self.read_txt(file_path)
                else: 
                    print(f"‚ö†Ô∏è Skipping unsupported file type: {filename}")
                    continue

                if not text.strip():
                    print(f"‚ö†Ô∏è No text extracted from {filename}, skipping.")
                    continue

                # Split into chunks
                chunks = self.text_splitter.split_text(text)
                total_chunks += len(chunks)

                for i, chunk in enumerate(chunks):
                    documents.append(LangchainDocument(
                        page_content=chunk, 
                        metadata={
                            "source": filename,
                            "chunk_index": i,
                            "document_type": ext
                        }
                    ))
                print(f"üìÑ Processed {filename} ‚Üí {len(chunks)} chunks")
            except Exception as e:
                print(f"‚ùå Error processing {filename}: {e}")
        
        print(f"‚úÖ Done! Total documents: {len(documents)} chunks across all files.")
        return documents

if __name__ == "__main__":
    processor = LegalDocumentProcessor()
    docs = processor.process_documents("data")  # Th∆∞ m·ª•c ch·ª©a PDF/DOCX/TXT
    print(f"Loaded {len(docs)} chunks.")
    if docs:
        print(docs[0].page_content[:300])  # Preview n·ªôi dung chunk ƒë·∫ßu ti√™n
        print(docs[0].metadata)
    else:
        print("No documents were processed.")